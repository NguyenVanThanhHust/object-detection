{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee3370de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.utils.data as data\n",
    "from PIL import Image\n",
    "from pycocotools.coco import COCO\n",
    "from typing import Any\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "117c0734",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CocoDataset(data.Dataset):\n",
    "    def __init__(self, root, annotation_file, transforms=None) -> None:\n",
    "        super().__init__()\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root (string): Directory path to the images.\n",
    "            annotation_file (string): Path to the COCO JSON annotation file.\n",
    "            transforms (callable, optional): Optional transforms to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        # Initialize the COCO API\n",
    "        self.coco = COCO(annotation_file)\n",
    "        self.image_ids = list(self.coco.imgs.keys())\n",
    "        self.root = root\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Returns the total number of images in the dataset.\n",
    "        \"\"\"\n",
    "        return len(self.image_ids)\n",
    "    \n",
    "    def __getitem__(self, idx) -> Any:\n",
    "        \"\"\"\n",
    "        Retrieves an image and its corresponding annotations for a given index.\n",
    "\n",
    "        Args:\n",
    "            index (int): Index of the image to retrieve.\n",
    "\n",
    "        Returns:\n",
    "            tuple: (image, target) where:\n",
    "                - image (PIL.Image): The image.\n",
    "                - target (dict): A dictionary containing the annotations.\n",
    "        \"\"\"\n",
    "        # Get image id\n",
    "        image_id = self.image_ids[idx]\n",
    "\n",
    "                # Load image metadata\n",
    "        image_info = self.coco.loadImgs(image_id)[0]\n",
    "        image_path = os.path.join(self.root, image_info['file_name'])\n",
    "\n",
    "        # Open the image using PIL\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "\n",
    "        # Load annotations for the current image\n",
    "        ann_ids = self.coco.getAnnIds(imgIds=image_id)\n",
    "        annotations = self.coco.loadAnns(ann_ids)\n",
    "\n",
    "        # Process annotations into a suitable format for PyTorch\n",
    "        boxes = []\n",
    "        labels = []\n",
    "        for ann in annotations:\n",
    "            # COCO bbox format is [x, y, width, height]\n",
    "            x, y, w, h = ann['bbox']\n",
    "            # Convert to [x_min, y_min, x_max, y_max] format for many models\n",
    "            x_min, y_min, x_max, y_max = x, y, x + w, y + h\n",
    "            boxes.append([x_min, y_min, x_max, y_max])\n",
    "            labels.append(ann['category_id'])\n",
    "\n",
    "        # Convert to PyTorch tensors\n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "        labels = torch.as_tensor(labels, dtype=torch.int64)\n",
    "\n",
    "        target = {}\n",
    "        target[\"boxes\"] = boxes\n",
    "        target[\"labels\"] = labels\n",
    "        target[\"image_id\"] = torch.tensor([image_id])\n",
    "        target[\"area\"] = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
    "        # iscrowd = torch.as_tensor([ann[\"iscrowd\"] for ann in annotations], dtype=torch.int64)\n",
    "        # target[\"iscrowd\"] = iscrowd\n",
    "\n",
    "        # Apply transformations if provided\n",
    "        if self.transforms is not None:\n",
    "            image, target = self.transforms(image, target)\n",
    "\n",
    "        return image, target\n",
    "    \n",
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    A custom collate function for object detection datasets.\n",
    "    It takes a list of samples (image, target) and returns a batch.\n",
    "    \"\"\"\n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "from torchvision import transforms\n",
    "\n",
    "# Define a simple transform pipeline\n",
    "# Note: For object detection, transforms like RandomCrop need to be applied\n",
    "# to both the image and the bounding boxes.\n",
    "# This simple example only shows basic image transforms.\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "965fb884",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# Create the custom dataset\n",
    "image_dir = \"Datasets/COCO/images/train2017\"\n",
    "annotation_file = \"Datasets/COCO/annotations/instances_train2017.json\"\n",
    "\n",
    "print(os.path.isdir(image_dir))\n",
    "print(os.path.isfile(annotation_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5bf6ff90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=28.64s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Caught TypeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/media/thanhnv154te/15a388e9-5b07-4e1c-b175-72e9839b5c872/workspace/dev_venv/lib/python3.10/site-packages/torch/utils/data/_utils/worker.py\", line 349, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n  File \"/media/thanhnv154te/15a388e9-5b07-4e1c-b175-72e9839b5c872/workspace/dev_venv/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 52, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/media/thanhnv154te/15a388e9-5b07-4e1c-b175-72e9839b5c872/workspace/dev_venv/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 52, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/tmp/ipykernel_2574243/3737425442.py\", line 74, in __getitem__\n    image, target = self.transforms(image, target)\nTypeError: Compose.__call__() takes 2 positional arguments but 3 were given\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 13\u001b[0m\n\u001b[1;32m      4\u001b[0m data_loader \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mDataLoader(\n\u001b[1;32m      5\u001b[0m     coco_dataset,\n\u001b[1;32m      6\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      9\u001b[0m     collate_fn\u001b[38;5;241m=\u001b[39mcollate_fn  \u001b[38;5;66;03m# Use the custom collate function\u001b[39;00m\n\u001b[1;32m     10\u001b[0m )\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Example of iterating through the DataLoader\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m images, targets \u001b[38;5;129;01min\u001b[39;00m data_loader:\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShape of images:\u001b[39m\u001b[38;5;124m\"\u001b[39m, images\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShape of targets:\u001b[39m\u001b[38;5;124m\"\u001b[39m, targets\u001b[38;5;241m.\u001b[39mshape)\n",
      "File \u001b[0;32m/media/thanhnv154te/15a388e9-5b07-4e1c-b175-72e9839b5c872/workspace/dev_venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:733\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    730\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    731\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    732\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 733\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    734\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    735\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    736\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    737\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    738\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    739\u001b[0m ):\n",
      "File \u001b[0;32m/media/thanhnv154te/15a388e9-5b07-4e1c-b175-72e9839b5c872/workspace/dev_venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1515\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1513\u001b[0m worker_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_task_info\u001b[38;5;241m.\u001b[39mpop(idx)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1514\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_rcvd_idx \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m-> 1515\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mworker_id\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/media/thanhnv154te/15a388e9-5b07-4e1c-b175-72e9839b5c872/workspace/dev_venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1550\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._process_data\u001b[0;34m(self, data, worker_idx)\u001b[0m\n\u001b[1;32m   1548\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_try_put_index()\n\u001b[1;32m   1549\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ExceptionWrapper):\n\u001b[0;32m-> 1550\u001b[0m     \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1551\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m/media/thanhnv154te/15a388e9-5b07-4e1c-b175-72e9839b5c872/workspace/dev_venv/lib/python3.10/site-packages/torch/_utils.py:750\u001b[0m, in \u001b[0;36mExceptionWrapper.reraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    746\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m    747\u001b[0m     \u001b[38;5;66;03m# If the exception takes multiple arguments or otherwise can't\u001b[39;00m\n\u001b[1;32m    748\u001b[0m     \u001b[38;5;66;03m# be constructed, don't try to instantiate since we don't know how to\u001b[39;00m\n\u001b[1;32m    749\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 750\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception\n",
      "\u001b[0;31mTypeError\u001b[0m: Caught TypeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/media/thanhnv154te/15a388e9-5b07-4e1c-b175-72e9839b5c872/workspace/dev_venv/lib/python3.10/site-packages/torch/utils/data/_utils/worker.py\", line 349, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n  File \"/media/thanhnv154te/15a388e9-5b07-4e1c-b175-72e9839b5c872/workspace/dev_venv/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 52, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/media/thanhnv154te/15a388e9-5b07-4e1c-b175-72e9839b5c872/workspace/dev_venv/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 52, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/tmp/ipykernel_2574243/3737425442.py\", line 74, in __getitem__\n    image, target = self.transforms(image, target)\nTypeError: Compose.__call__() takes 2 positional arguments but 3 were given\n"
     ]
    }
   ],
   "source": [
    "coco_dataset = CocoDataset(root=image_dir, annotation_file=annotation_file, transforms=transform)\n",
    "\n",
    "# Create the DataLoader\n",
    "data_loader = data.DataLoader(\n",
    "    coco_dataset,\n",
    "    batch_size=2,\n",
    "    shuffle=True,\n",
    "    num_workers=4,\n",
    "    collate_fn=collate_fn  # Use the custom collate function\n",
    ")\n",
    "\n",
    "# Example of iterating through the DataLoader\n",
    "for images, targets in data_loader:\n",
    "    print(\"Shape of images:\", images.shape)\n",
    "    print(\"Shape of targets:\", targets.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f077271c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
